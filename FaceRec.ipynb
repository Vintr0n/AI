{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a1290a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# Set the input and output directories\n",
    "input_dir = 'photos/Input/x'\n",
    "output_dir = 'photos/Output/x'\n",
    "\n",
    "# Set the desired image size\n",
    "img_size = (224, 224)\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Get a list of all image files in the input directory\n",
    "img_files = glob.glob(os.path.join(input_dir, '*.jpg'))\n",
    "\n",
    "# Loop through each image file\n",
    "for img_file in img_files:\n",
    "    # Load the image\n",
    "    img = cv2.imread(img_file)\n",
    "    \n",
    "    # Resize the image\n",
    "    img = cv2.resize(img, img_size)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Normalize the pixel values\n",
    "    normalized_img = gray_img / 255.0\n",
    "    \n",
    "    # Get the filename of the image file\n",
    "    filename = os.path.basename(img_file)\n",
    "    \n",
    "    # Save the pre-processed image to the output directory\n",
    "    output_file = os.path.join(output_dir, filename)\n",
    "    _, encoded_img = cv2.imencode('.jpg', normalized_img * 255.0)\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(encoded_img)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "be62882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# Set the input and output directories\n",
    "input_dir = 'photos/Input/y'\n",
    "output_dir = 'photos/Output/y'\n",
    "\n",
    "# Set the desired image size\n",
    "img_size = (224, 224)\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Get a list of all image files in the input directory\n",
    "img_files = glob.glob(os.path.join(input_dir, '*.jpg'))\n",
    "\n",
    "# Loop through each image file\n",
    "for img_file in img_files:\n",
    "    # Load the image\n",
    "    img = cv2.imread(img_file)\n",
    "    \n",
    "    # Resize the image\n",
    "    img = cv2.resize(img, img_size)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Normalize the pixel values\n",
    "    normalized_img = gray_img / 255.0\n",
    "    \n",
    "    # Get the filename of the image file\n",
    "    filename = os.path.basename(img_file)\n",
    "    \n",
    "    # Save the pre-processed image to the output directory\n",
    "    output_file = os.path.join(output_dir, filename)\n",
    "    _, encoded_img = cv2.imencode('.jpg', normalized_img * 255.0)\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(encoded_img)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f56f9e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 images belonging to 2 classes.\n",
      "Found 22 images belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 1s 792ms/step - loss: 1.8636 - accuracy: 0.5455 - val_loss: 6.4639 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 1s 733ms/step - loss: 6.3966 - accuracy: 0.4545 - val_loss: 1.6202 - val_accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 1s 728ms/step - loss: 1.9837 - accuracy: 0.5909 - val_loss: 1.7535 - val_accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 1s 747ms/step - loss: 1.0990 - accuracy: 0.5909 - val_loss: 0.6381 - val_accuracy: 0.5909\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 1s 716ms/step - loss: 0.4663 - accuracy: 0.8636 - val_loss: 0.7266 - val_accuracy: 0.5455\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 1s 717ms/step - loss: 0.4394 - accuracy: 0.7273 - val_loss: 0.6828 - val_accuracy: 0.5455\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 1s 756ms/step - loss: 0.3972 - accuracy: 0.7727 - val_loss: 0.6006 - val_accuracy: 0.7273\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 1s 771ms/step - loss: 0.4152 - accuracy: 0.7727 - val_loss: 0.6196 - val_accuracy: 0.6364\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 1s 777ms/step - loss: 0.2138 - accuracy: 1.0000 - val_loss: 0.7158 - val_accuracy: 0.6364\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 1s 760ms/step - loss: 0.1738 - accuracy: 0.9545 - val_loss: 0.5668 - val_accuracy: 0.7273\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 1s 788ms/step - loss: 0.1162 - accuracy: 0.9091 - val_loss: 0.5816 - val_accuracy: 0.6818\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 1s 797ms/step - loss: 0.0711 - accuracy: 1.0000 - val_loss: 0.7126 - val_accuracy: 0.7273\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 1s 734ms/step - loss: 0.0413 - accuracy: 1.0000 - val_loss: 0.9061 - val_accuracy: 0.6818\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 1s 703ms/step - loss: 0.0450 - accuracy: 1.0000 - val_loss: 0.7348 - val_accuracy: 0.7273\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 1s 766ms/step - loss: 0.0232 - accuracy: 1.0000 - val_loss: 0.8987 - val_accuracy: 0.6818\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 1s 734ms/step - loss: 0.0836 - accuracy: 0.9545 - val_loss: 0.8272 - val_accuracy: 0.6818\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 1s 734ms/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 1.1533 - val_accuracy: 0.6818\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 1s 719ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 1.6547 - val_accuracy: 0.6364\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 1s 719ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 2.0076 - val_accuracy: 0.5909\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 1s 735ms/step - loss: 0.0824 - accuracy: 0.9545 - val_loss: 1.3833 - val_accuracy: 0.6364\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 1s 734ms/step - loss: 0.0243 - accuracy: 1.0000 - val_loss: 0.8504 - val_accuracy: 0.6364\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 1s 719ms/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 0.7198 - val_accuracy: 0.6818\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 1s 750ms/step - loss: 0.0577 - accuracy: 0.9545 - val_loss: 0.7139 - val_accuracy: 0.6818\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 1s 719ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.7470 - val_accuracy: 0.6818\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 1s 750ms/step - loss: 0.0345 - accuracy: 1.0000 - val_loss: 0.7257 - val_accuracy: 0.6818\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 1s 781ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.7685 - val_accuracy: 0.7727\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 1s 750ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.8313 - val_accuracy: 0.6818\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 1s 734ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.8742 - val_accuracy: 0.6818\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 1s 735ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.9403 - val_accuracy: 0.7273\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 1s 725ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.9992 - val_accuracy: 0.7273\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 1s 728ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.9859 - val_accuracy: 0.7273\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 1s 775ms/step - loss: 4.7117e-04 - accuracy: 1.0000 - val_loss: 1.0067 - val_accuracy: 0.7273\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 1s 772ms/step - loss: 6.1173e-04 - accuracy: 1.0000 - val_loss: 1.0658 - val_accuracy: 0.6818\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 1s 771ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 1.2773 - val_accuracy: 0.6364\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 1s 753ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 1.4850 - val_accuracy: 0.6364\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 1s 775ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 1.2873 - val_accuracy: 0.6364\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 1s 760ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 1.2446 - val_accuracy: 0.7273\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 1s 756ms/step - loss: 3.0943e-04 - accuracy: 1.0000 - val_loss: 1.3558 - val_accuracy: 0.7273\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 1s 740ms/step - loss: 5.0999e-04 - accuracy: 1.0000 - val_loss: 1.5897 - val_accuracy: 0.6818\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 1s 778ms/step - loss: 6.9378e-05 - accuracy: 1.0000 - val_loss: 1.9438 - val_accuracy: 0.7273\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 1s 751ms/step - loss: 0.0717 - accuracy: 0.9545 - val_loss: 1.8592 - val_accuracy: 0.5909\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 1s 722ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 2.2214 - val_accuracy: 0.5455\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 1s 757ms/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 2.1640 - val_accuracy: 0.5455\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 1s 761ms/step - loss: 0.0603 - accuracy: 1.0000 - val_loss: 1.9029 - val_accuracy: 0.5455\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 1s 728ms/step - loss: 0.0541 - accuracy: 1.0000 - val_loss: 1.5924 - val_accuracy: 0.5909\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 1s 736ms/step - loss: 0.0370 - accuracy: 1.0000 - val_loss: 1.4425 - val_accuracy: 0.5909\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 1s 765ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.4264 - val_accuracy: 0.5909\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 1s 738ms/step - loss: 0.0263 - accuracy: 1.0000 - val_loss: 1.3625 - val_accuracy: 0.6364\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 1s 741ms/step - loss: 0.0834 - accuracy: 0.9545 - val_loss: 1.5240 - val_accuracy: 0.5909\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 1s 721ms/step - loss: 0.0376 - accuracy: 0.9545 - val_loss: 1.7065 - val_accuracy: 0.5909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11d4b7438b0>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import numpy as np\n",
    "\n",
    "# Define the image size and directory\n",
    "img_size = (224, 224)\n",
    "train_dir = 'photos/Output'\n",
    "\n",
    "# Load and preprocess the images and labels using an ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.5)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=img_size,\n",
    "        batch_size=11,\n",
    "        class_mode='categorical',  # Use categorical mode for two or more classes\n",
    "        subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=img_size,\n",
    "        batch_size=11,\n",
    "        class_mode='categorical',\n",
    "        subset='validation')\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a convolutional layer with 32 filters, a 3x3 kernel, and relu activation function\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "\n",
    "# Add a max pooling layer with a 2x2 pool size\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Add another convolutional layer with 64 filters, a 3x3 kernel, and relu activation function\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Add another max pooling layer with a 2x2 pool size\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten the output of the previous layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add a dense layer with 64 units and relu activation function\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Add a dropout layer with a rate of 0.5\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add a dense layer with 2 units and softmax activation function for multiple classes\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model with categorical cross-entropy loss and Adam optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with 50 epochs\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d3d9b631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Chloe': 0, 'Hannah': 1}\n",
      "Number of examples in class 1: 22\n",
      "Number of examples in class 2: 22\n"
     ]
    }
   ],
   "source": [
    "class_indices = train_generator.class_indices\n",
    "print(class_indices)\n",
    "\n",
    "num_class1 = len(os.listdir(os.path.join(train_dir, list(class_indices.keys())[0])))\n",
    "num_class2 = len(os.listdir(os.path.join(train_dir, list(class_indices.keys())[1])))\n",
    "\n",
    "print(\"Number of examples in class 1:\", num_class1)\n",
    "print(\"Number of examples in class 2:\", num_class2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "02cc82c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load image\n",
    "img = cv2.imread('photos/output/train/train.jpg')\n",
    "\n",
    "# Check pixel values\n",
    "if (img == img[0,0]).all():\n",
    "    print(\"There is a problem with the loading process\")\n",
    "else:\n",
    "    print(\"Image loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4f5e57c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('FaceRec.h5')\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open('FaceRec.json', 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8bd1f535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_22 (Conv2D)          (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_22 (MaxPoolin  (None, 111, 111, 32)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_23 (MaxPoolin  (None, 54, 54, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 186624)            0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 64)                11944000  \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,963,522\n",
      "Trainable params: 11,963,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('FaceRec.h5')\n",
    "\n",
    "# Print the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5b1844a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 235ms/step\n",
      "0.9761763\n",
      "0.99999976\n",
      "0.9999654\n",
      "0.99999666\n",
      "0.97399163\n",
      "0.9999994\n",
      "0.9999999\n",
      "0.9999932\n",
      "0.997869\n",
      "0.9595778\n",
      "0.9995029\n",
      "0.99358433\n",
      "0.85356706\n",
      "0.996698\n",
      "0.99996364\n",
      "0.9991032\n",
      "0.9998653\n",
      "0.9999809\n",
      "0.6492236\n",
      "0.8305607\n",
      "0.9909599\n",
      "0.5359824\n",
      "0.71297723\n",
      "0.70296735\n",
      "0.9782336\n",
      "0.5552312\n",
      "0.99932206\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define the preprocess_image function\n",
    "def preprocess_image(img):\n",
    "    \"\"\"\n",
    "    Preprocesses an image for input into a machine learning model.\n",
    "    \"\"\"\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "# Load the test images\n",
    "test_images = []\n",
    "for i in range(1, 28):\n",
    "    img_path = f\"photos/test/test{i}.jpg\"\n",
    "    img = cv2.imread(img_path)\n",
    "    img = preprocess_image(img)\n",
    "    test_images.append(img)\n",
    "\n",
    "# Convert the test images to a numpy array\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "# Reshape the test images to match the expected shape of the model\n",
    "test_images = np.squeeze(test_images, axis=1)\n",
    "\n",
    "# Make predictions for each image\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "# Get the class labels from the directory names\n",
    "classes = os.listdir('photos/Output')\n",
    "\n",
    "# Print the predicted class and probability for each image\n",
    "for i, prediction in enumerate(predictions):\n",
    "    class_idx = np.argmax(prediction)\n",
    "    probability = prediction[class_idx]\n",
    "    class_name = classes[class_idx]\n",
    "    #print(f\"Test image {i+1}: predicted class '{class_name}' with probability {probability:.2f}\")\n",
    "    print(class_name)\n",
    "    print(probability)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
